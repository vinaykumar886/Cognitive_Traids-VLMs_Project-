{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcKqqJVQsLJJ"
      },
      "source": [
        "**This is the VLM which works on the dataset related to majorly animals and some normal things like car etc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "IyB8ZAwAomsY",
        "outputId": "dbcd8f87-ba02-4a3b-f1f4-a602c6e32162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Visual Search Engine...\n",
            " Initializing on CPU device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Downloading CIFAR-10 dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " Saving images: 100%|██████████| 500/500 [00:00<00:00, 546.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-10 subset prepared (500 images)\n",
            "Building search index...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing images: 100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search index ready with 500 images\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a1d13082d72f5d292d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a1d13082d72f5d292d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#pip install transformers pillow faiss-cpu numpy torch gradio torchvision --quiet\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFile\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import faiss\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import gradio as gr\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class VisualSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\" Initializing on {self.device.upper()} device\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            self.model = CLIPModel.from_pretrained(\n",
        "                \"openai/clip-vit-base-patch32\",\n",
        "                device_map=\"auto\"\n",
        "            ).to(self.device).eval()\n",
        "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "            print(\"Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Model loading failed: {e}\")\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.index = None\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        \"\"\"Download and prepare the CIFAR-10 dataset subset\"\"\"\n",
        "        print(\"Downloading CIFAR-10 dataset...\")\n",
        "        try:\n",
        "            os.makedirs(\"cifar10_images\", exist_ok=True)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((128, 128)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "            cifar10 = datasets.CIFAR10(\n",
        "                root='./data',\n",
        "                train=True,\n",
        "                download=True,\n",
        "                transform=transform\n",
        "            )\n",
        "\n",
        "            for idx in tqdm(range(500), desc=\" Saving images\"):\n",
        "                image, _ = cifar10[idx]\n",
        "                img = transforms.ToPILImage()(image)\n",
        "                img.save(f\"cifar10_images/{idx:04d}.jpg\", quality=95)\n",
        "\n",
        "            print(\"CIFAR-10 subset prepared (500 images)\")\n",
        "            return \"cifar10_images\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Using fallback images: {e}\")\n",
        "            self.create_fallback_images()\n",
        "            return \"fallback_images\"\n",
        "\n",
        "    def create_fallback_images(self):\n",
        "        \"\"\"Generate fallback images if dataset download fails\"\"\"\n",
        "        os.makedirs(\"fallback_images\", exist_ok=True)\n",
        "\n",
        "        colors = ['red', 'blue', 'green']\n",
        "        for i, color in enumerate(colors):\n",
        "            img = Image.new('RGB', (128, 128), color)\n",
        "            img.save(f'fallback_images/example{i+1}.jpg')\n",
        "\n",
        "    def build_index(self, image_folder):\n",
        "        \"\"\"Build FAISS index with error handling\"\"\"\n",
        "        print(\"Building search index...\")\n",
        "\n",
        "        self.image_paths = sorted([\n",
        "            os.path.join(image_folder, f)\n",
        "            for f in os.listdir(image_folder)\n",
        "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "        ][:500])\n",
        "\n",
        "        if not self.image_paths:\n",
        "            raise RuntimeError(\"No valid images found in directory\")\n",
        "\n",
        "        embeddings = []\n",
        "        valid_paths = []\n",
        "\n",
        "        for path in tqdm(self.image_paths, desc=\"Processing images\"):\n",
        "            try:\n",
        "                with Image.open(path).convert(\"RGB\") as img:\n",
        "                    inputs = self.processor(\n",
        "                        images=img,\n",
        "                        return_tensors=\"pt\"\n",
        "                    ).to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        embedding = self.model.get_image_features(**inputs)\n",
        "\n",
        "                    embeddings.append(embedding.cpu().numpy())\n",
        "                    valid_paths.append(path)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipped {os.path.basename(path)}: {str(e)}\")\n",
        "\n",
        "        if not embeddings:\n",
        "            raise RuntimeError(\"No valid embeddings generated\")\n",
        "\n",
        "        embeddings = np.vstack(embeddings).astype('float32')\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "        self.index.add(embeddings)\n",
        "        self.image_paths = valid_paths\n",
        "        print(f\"Search index ready with {len(self.image_paths)} images\")\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"Unified search for both text and image queries\"\"\"\n",
        "        try:\n",
        "\n",
        "            if isinstance(query, (str, np.ndarray, Image.Image)):\n",
        "                embedding = self._get_query_embedding(query)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid query type\")\n",
        "\n",
        "            if embedding is None:\n",
        "                return []\n",
        "\n",
        "            embedding = embedding.astype('float32')\n",
        "            faiss.normalize_L2(embedding)\n",
        "            _, indices = self.index.search(embedding, top_k)\n",
        "\n",
        "            return [self.image_paths[i] for i in indices[0] if i < len(self.image_paths)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _get_query_embedding(self, query):\n",
        "        \"\"\"Generate embedding for different query types\"\"\"\n",
        "        try:\n",
        "            if isinstance(query, str):\n",
        "                if os.path.isfile(query):\n",
        "                    with Image.open(query) as img:\n",
        "                        return self._embed_image(img)\n",
        "                return self._embed_text(query)\n",
        "\n",
        "            if isinstance(query, np.ndarray):\n",
        "                query = Image.fromarray(query)\n",
        "\n",
        "            if isinstance(query, Image.Image):\n",
        "                return self._embed_image(query)\n",
        "\n",
        "            raise ValueError(\"Unsupported query type\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Embedding error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _embed_image(self, image):\n",
        "        \"\"\"Generate image embeddings\"\"\"\n",
        "        inputs = self.processor(\n",
        "            images=image.convert(\"RGB\"),\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            return self.model.get_image_features(**inputs).cpu().numpy()\n",
        "\n",
        "    def _embed_text(self, text):\n",
        "        \"\"\"Generate text embeddings\"\"\"\n",
        "        inputs = self.processor(\n",
        "            text=text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            return self.model.get_text_features(**inputs).cpu().numpy()\n",
        "\n",
        "def create_gui(engine):\n",
        "    \"\"\"Create Gradio interface with improved layout\"\"\"\n",
        "    with gr.Blocks(title=\"Visual Search Engine\", theme=\"soft\") as interface:\n",
        "        gr.Markdown(\"#  Visual Search Engine\")\n",
        "\n",
        "        with gr.Tab(\"Text Search\"):\n",
        "            gr.Markdown(\"###  Search using text description\")\n",
        "            with gr.Row():\n",
        "                text_input = gr.Textbox(label=\"Search Query\",\n",
        "                                       placeholder=\"Enter text (e.g., 'red car', 'animal')\")\n",
        "                text_search = gr.Button(\"Search\", variant=\"primary\")\n",
        "            gr.Examples([\"red truck\", \"bird flying\", \"green frog\"],\n",
        "                       inputs=text_input)\n",
        "            text_output = gr.Gallery(label=\"Results\", columns=5)\n",
        "\n",
        "        with gr.Tab(\"Image Search\"):\n",
        "            gr.Markdown(\"### Search using image\")\n",
        "            with gr.Row():\n",
        "                image_input = gr.Image(label=\"Upload Image\", type=\"pil\")\n",
        "                image_search = gr.Button(\"Search\", variant=\"primary\")\n",
        "            gr.Examples([\"fallback_images/example1.jpg\"],\n",
        "                       inputs=image_input)\n",
        "            image_output = gr.Gallery(label=\"Results\", columns=5)\n",
        "\n",
        "        text_search.click(\n",
        "            lambda q: engine.search(q),\n",
        "            inputs=text_input,\n",
        "            outputs=text_output\n",
        "        )\n",
        "        image_search.click(\n",
        "            lambda img: engine.search(img),\n",
        "            inputs=image_input,\n",
        "            outputs=image_output\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "def main():\n",
        "    engine = VisualSearchEngine()\n",
        "    image_dir = engine.prepare_dataset()\n",
        "    engine.build_index(image_dir)\n",
        "    create_gui(engine).launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Visual Search Engine...\")\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
